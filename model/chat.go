package model

import "github.com/iimeta/go-openai"

// ChatCompletionRequest represents a request structure for chat completion API.
type ChatCompletionRequest struct {
	Model               string                               `json:"model"`
	Messages            []ChatCompletionMessage              `json:"messages"`
	MaxTokens           int                                  `json:"max_tokens,omitempty"`
	MaxCompletionTokens int                                  `json:"max_completion_tokens,omitempty"`
	Temperature         float32                              `json:"temperature,omitempty"`
	TopP                float32                              `json:"top_p,omitempty"`
	TopK                int                                  `json:"top_k,omitempty"`
	N                   int                                  `json:"n,omitempty"`
	Stream              bool                                 `json:"stream,omitempty"`
	Stop                []string                             `json:"stop,omitempty"`
	PresencePenalty     float32                              `json:"presence_penalty,omitempty"`
	ResponseFormat      *openai.ChatCompletionResponseFormat `json:"response_format,omitempty"`
	Seed                *int                                 `json:"seed,omitempty"`
	FrequencyPenalty    float32                              `json:"frequency_penalty,omitempty"`
	// LogitBias is must be a token id string (specified by their token ID in the tokenizer), not a word string.
	// incorrect: `"logit_bias":{"You": 6}`, correct: `"logit_bias":{"1639": 6}`
	// refs: https://platform.openai.com/docs/api-reference/chat/create#chat/create-logit_bias
	LogitBias map[string]int `json:"logit_bias,omitempty"`
	// LogProbs indicates whether to return log probabilities of the output tokens or not.
	// If true, returns the log probabilities of each output token returned in the content of message.
	// This option is currently not available on the gpt-4-vision-preview model.
	LogProbs bool `json:"logprobs,omitempty"`
	// TopLogProbs is an integer between 0 and 5 specifying the number of most likely tokens to return at each
	// token position, each with an associated log probability.
	// logprobs must be set to true if this parameter is used.
	TopLogProbs  int                         `json:"top_logprobs,omitempty"`
	User         string                      `json:"user,omitempty"`
	Functions    []openai.FunctionDefinition `json:"functions,omitempty"`
	FunctionCall any                         `json:"function_call,omitempty"`
	Tools        []openai.Tool               `json:"tools,omitempty"`
	// This can be either a string or an ToolChoice object.
	ToolChoice any `json:"tool_choice,omitempty"`
	// Options for streaming response. Only set this when you set stream: true.
	StreamOptions *openai.StreamOptions `json:"stream_options,omitempty"`
	// Disable the default behavior of parallel tool calls by setting it: false.
	ParallelToolCalls any `json:"parallel_tool_calls,omitempty"`
}

// ChatCompletionResponse represents a response structure for chat completion API.
type ChatCompletionResponse struct {
	ID                string                    `json:"id"`
	Object            string                    `json:"object"`
	Created           int64                     `json:"created"`
	Model             string                    `json:"model"`
	Choices           []ChatCompletionChoice    `json:"choices"`
	Usage             *Usage                    `json:"usage"`
	SystemFingerprint string                    `json:"system_fingerprint,omitempty"`
	PromptAnnotations []openai.PromptAnnotation `json:"prompt_annotations,omitempty"`
	ResponseBytes     []byte                    `json:"-"`
	ConnTime          int64                     `json:"-"`
	Duration          int64                     `json:"-"`
	TotalTime         int64                     `json:"-"`
	Error             error                     `json:"-"`
}

type ChatCompletionMessage struct {
	Role         string                   `json:"role"`
	Content      any                      `json:"content"`
	MultiContent []openai.ChatMessagePart `json:"-"`

	// This property isn't in the official documentation, but it's in
	// the documentation for the official library for python:
	// - https://github.com/openai/openai-python/blob/main/chatml.md
	// - https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
	Name string `json:"name,omitempty"`

	FunctionCall *openai.FunctionCall `json:"function_call,omitempty"`

	// For Role=assistant prompts this may be set to the tool calls generated by the model, such as function calls.
	ToolCalls []openai.ToolCall `json:"tool_calls,omitempty"`

	// For Role=tool prompts this should be set to the ID given in the assistant's prior request to call a tool.
	ToolCallID string `json:"tool_call_id,omitempty"`
}

type ChatCompletionChoice struct {
	Index        int                                     `json:"index"`
	Message      *openai.ChatCompletionMessage           `json:"message,omitempty"`
	Delta        *openai.ChatCompletionStreamChoiceDelta `json:"delta,omitempty"`
	LogProbs     *openai.LogProbs                        `json:"logprobs,omitempty"`
	FinishReason openai.FinishReason                     `json:"finish_reason"`
	//ContentFilterResults *openai.ContentFilterResults            `json:"content_filter_results,omitempty"`
}

// Usage Represents the total token usage per request to OpenAI.
type Usage struct {
	PromptTokens            int `json:"prompt_tokens"`
	CompletionTokens        int `json:"completion_tokens"`
	TotalTokens             int `json:"total_tokens"`
	CompletionTokensDetails struct {
		ReasoningTokens int `json:"reasoning_tokens"`
	} `json:"completion_tokens_details"`
}
